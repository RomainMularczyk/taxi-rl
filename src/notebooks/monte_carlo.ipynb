{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d294846-f06e-4988-abee-090e54035d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import networkx as nx\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, TypedDict, Union\n",
    "from gymnasium.wrappers.time_limit import TimeLimit as GameEnv\n",
    "from networkx.classes.graph import Graph\n",
    "# Gymnasium wrappers\n",
    "os.chdir(\"..\")\n",
    "from lib.models.Action import Action\n",
    "from lib.models.Destination import Destination\n",
    "from lib.models.EnvironmentInfo import EnvironmentInfo\n",
    "from lib.environment.environment import GameEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e93be-1ccd-44c3-b467-28c811ad91eb",
   "metadata": {},
   "source": [
    "# Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "128fd264-6c2e-49ef-ba24-d56b82b28498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplePolicy:\n",
    "    \"\"\"\n",
    "    Sample randomly the next action to take.\n",
    "    \"\"\"\n",
    "    def pick_action(env: GameEnv):\n",
    "        \"\"\"\n",
    "        Provide the action for the \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: GameEnv\n",
    "            The game environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        return env.action_space.sample()\n",
    "\n",
    "class MonteCarloPolicy:\n",
    "    def pick_action():\n",
    "        pass\n",
    "\n",
    "Policies = Union[SamplePolicy, MonteCarloPolicy]\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, policy: Policies):\n",
    "        self.policy = policy\n",
    "    \n",
    "    def pick_action(env: GameEnv):\n",
    "        return self.policy.pick_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c84126-2667-488f-9682-ee683f07cfa6",
   "metadata": {},
   "source": [
    "# Underlying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "ca9b1236-6d71-4003-912b-f48fb7857169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(n: int, p: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the expected value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Number of outcomes.\n",
    "    p: float\n",
    "        Probability of each outcome.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The expected value.\n",
    "    \"\"\"\n",
    "    e = [i * p for i in range(n + 1)]\n",
    "    e = np.array(e)\n",
    "    return e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "a5ccf7b5-9f0f-4ae6-9494-bd226b8d2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_reward(\n",
    "    rs: float, \n",
    "    r: float, \n",
    "    gamma: float | bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the cumulative rewards.\n",
    "\n",
    "    Parmaters\n",
    "    ---------\n",
    "    rs: float\n",
    "        The current accumulated rewards.\n",
    "    r: float\n",
    "        The next reward value.\n",
    "    gamma: float | bool, default=False\n",
    "        The discount factor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accumulated rewards, taking into account the next reward value.\n",
    "    \"\"\"\n",
    "    if gamma:\n",
    "        return rs + (gamma * r)\n",
    "    return rs + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "094d0c1d-b6b1-4a3f-b1d5-0db90334904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(rs: float, s, a):\n",
    "    \"\"\"\n",
    "    Compute the expected return after taking an action (a), starting\n",
    "    from a given state (s).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rs: float\n",
    "        The cumulated rewards.\n",
    "    s: float\n",
    "        The next state.\n",
    "    a: float\n",
    "        The next action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The expected value of the cumulated rewards.\n",
    "    \"\"\"\n",
    "    return expected_value(cumulative_reward(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "05c036cc-6401-4750-8455-dab25d6f04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman(gamma: float, r: float, s: float, a: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the expected value of the reward for the next step\n",
    "    additionned to the cumulated rewards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float\n",
    "        The discount factor.\n",
    "    r: float\n",
    "        The reward for the next step taken.\n",
    "    s: float\n",
    "        The reward for being in the current step.\n",
    "    a: float\n",
    "        The reward.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The reward propagated through the previous iterations.\n",
    "    \"\"\"\n",
    "    return expected_value(r + gamma * q_function(s, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "0bb3c274-f8fa-489f-90b4-be0fc41a7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q_table(observation_space: int, actions: int):\n",
    "    \"\"\"\n",
    "    Create the Q-table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observation_space: int\n",
    "        The cardinal of the set of all possible states.\n",
    "    actions: int\n",
    "        The cardinal of the set of all possible actions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The Q-table.\n",
    "    \"\"\"\n",
    "    return np.zeros((observation_space, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "d4fafe38-e656-4220-a96b-7bf94973bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = init_q_table(500, 6)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f8ae4-a83a-43cf-b018-fe8775f80139",
   "metadata": {},
   "source": [
    "# 1. Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "b063dbdf-e0d6-4533-b709-b19521406d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode=\"ansi\")\n",
    "env = GameEnvironment(env=env, reward=0, policy=SamplePolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "f4ad0dd7-813f-4a17-9274-5fca635a2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "eefb3ad9-f0f6-4fa4-a686-d603da102bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = env.reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5216e8af-43fc-4a2b-b579-d1303637533e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)}\n",
      "<class 'dict'>\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romainm/Library/Caches/pypoetry/virtualenvs/aia-902-RiSBwf3h-py3.11/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.s to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.s` for environment variables or `env.get_wrapper_attr('s')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.do_step(0, render=True)\n",
    "rewards += env.reward\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88126d23-ff70-46b4-926f-93887a14dbca",
   "metadata": {},
   "source": [
    "# 2. Non-markovian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5900a-88bb-47eb-a50f-14455a3eb064",
   "metadata": {},
   "source": [
    "Take into account where we come from :\n",
    "- We don't want to go back from the place where we picked up the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd833f6-f7ec-4df4-975a-5ef0a322deab",
   "metadata": {},
   "source": [
    "# 2. Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e165c62-fc72-4cd2-aecc-d854762e2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6f767f6-4894-4b5f-94db-314a0195ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo(GameEnvironment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: GameEnv,\n",
    "        reward: int, \n",
    "        graph: Graph,\n",
    "        seed: int = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            reward=reward,\n",
    "            seed=seed\n",
    "        )\n",
    "        self.graph = graph\n",
    "    \n",
    "    def mcts(self):\n",
    "        self.do_step(previous_rewards=self.reward)\n",
    "        graph.add_node(self.state)\n",
    "        # 1. Pick a node given the policy\n",
    "        # 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "508303e2-9ad2-4a9d-8738-6c4d03c29a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo = MonteCarlo(env=env, reward=0, graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49bd71ba-5401-45e7-8f9a-adef13675cef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmonte_carlo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m, in \u001b[0;36mMonteCarlo.mcts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmcts\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_step(previous_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/aia-902-RiSBwf3h-py3.11/lib/python3.11/site-packages/networkx/classes/graph.py:556\u001b[0m, in \u001b[0;36mGraph.add_node\u001b[0;34m(self, node_for_adding, **attr)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_for_adding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a single node `node_for_adding` and update node attributes.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    doesn't change on mutables.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnode_for_adding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m:\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m node_for_adding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone cannot be a node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "monte_carlo.mcts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e04b8-b1d5-4cf3-9f4a-1d258f318797",
   "metadata": {},
   "source": [
    "# 3. Deep-Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8cb59-624f-4958-9e97-5411cd78afb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ff2537-eae8-4bec-9213-e2a31efd88c1",
   "metadata": {},
   "source": [
    "# Analytic solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae5045-2642-4749-8d6b-f59c6ffa4d46",
   "metadata": {},
   "source": [
    "In order to solve the problem using an analytic solution, given the restricted size of the environment (500), we could explore all the possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10005bd-9b98-4871-8a80-6fd2cd5c534b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
